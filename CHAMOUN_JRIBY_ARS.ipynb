{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "ww1QXa7esAoU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "def policy_function(M, state, delta, nu, v2, sigma=None, mu=None):\n",
        "    if v2:\n",
        "        # In v2, action probability is computed by adjusting the policy weights (M) by a perturbation (nu * delta),\n",
        "        # then scaling by the inverse of the covariance matrix (sigma) and centering by the mean state (mu).\n",
        "        # The resulting value (action_prob) represents the likelihood of choosing one action over another.\n",
        "        action_prob = (M + nu * delta) @ np.linalg.inv(sigma) @ (state - mu)\n",
        "    else:\n",
        "        # The action probability is computed by adjusting the policy weights (M) by a perturbation (nu * delta)\n",
        "        # and then applying these adjusted weights to the current state.\n",
        "        action_prob = (M + nu * delta) @ state\n",
        "\n",
        "    # Determine the action based on the sign of the action probability.\n",
        "    # If action_prob is positive or zero, the action will be 1; otherwise, it will be 0.\n",
        "    action = int(action_prob >= 0)\n",
        "\n",
        "    # Return the computed action.\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRDuzui6hHN-"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(policy, env, max_steps=1000):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    \n",
        "    # Continue taking steps in the environment until the episode ends ('done' becomes True)\n",
        "    # or the number of steps reaches the specified maximum ('max_steps').\n",
        "    while not done and steps < max_steps:\n",
        "        action = policy(state)  # Determine the action to take in the current state by calling the provided 'policy' function.\n",
        "        state, reward, done, _ = env.step(action)  # Apply the action in the environment.\n",
        "\n",
        "        total_reward += reward  # Add the reward received from the last action to the total accumulated reward.\n",
        "        steps += 1  # Increment the step counter.\n",
        "        \n",
        "    # Return the total accumulated reward for the episode.\n",
        "    return total_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozIvChBThC7t"
      },
      "outputs": [],
      "source": [
        "def augmented_random_search(env, alpha, N, nu, b, max_episodes, v2=False):\n",
        "    n = env.observation_space.shape[0]\n",
        "\n",
        "    M = np.zeros(n)     # Initialize policy weights as zeros\n",
        "    mu = np.zeros(n)    # Initialize mean state as zeros\n",
        "    sigma = np.eye(n)   # Initialize covariance matrix as identity matrix\n",
        "\n",
        "    for _ in range(max_episodes):\n",
        "        deltas = np.random.randn(N, n)  # Generate N random perturbations for policy weights\n",
        "\n",
        "        rewards_plus = []   # To store rewards when adding perturbations\n",
        "        rewards_minus = []  # To store rewards when subtracting perturbations\n",
        "\n",
        "        # Evaluate policy for each perturbation and its negation\n",
        "        for delta in deltas:\n",
        "            # Define perturbed policies\n",
        "            policy_plus = lambda s, d=delta: policy_function(M, s, d, nu, v2, sigma, mu)\n",
        "            policy_minus = lambda s, d=delta: policy_function(M, s, -d, nu, v2, sigma, mu)\n",
        "\n",
        "            # Evaluate each perturbed policy and store rewards\n",
        "            rewards_plus.append(evaluate_policy(policy_plus, env))\n",
        "            rewards_minus.append(evaluate_policy(policy_minus, env))\n",
        "\n",
        "        # Select the top b perturbations based on max rewards obtained from perturbed policies\n",
        "        scores = list(zip(deltas, rewards_plus, rewards_minus))\n",
        "        scores.sort(key=lambda x: max(x[1], x[2]), reverse=True)\n",
        "        top_scores = scores[:b]\n",
        "\n",
        "        # Update policy weights using the top b perturbations\n",
        "        update_step = np.zeros(n)\n",
        "        sigma_rewards = np.std([r for _, r_plus, r_minus in top_scores for r in (r_plus, r_minus)]) + 1e-4\n",
        "        for delta, reward_plus, reward_minus in top_scores:\n",
        "            update_step += (reward_plus - reward_minus) * delta\n",
        "\n",
        "        M += (alpha / (b * sigma_rewards)) * update_step  # Apply update to policy weights\n",
        "\n",
        "        # We haven't done this part yet:\n",
        "        # V2 : Set μj+1, Σj+1 to be the mean and covariance of the 2NH(j + 1) states encountered from the start of training.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
