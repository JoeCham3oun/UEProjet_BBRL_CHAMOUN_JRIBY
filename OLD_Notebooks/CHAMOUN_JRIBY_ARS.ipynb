{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww1QXa7esAoU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "\n",
        "def policy_function(M, state, delta, nu, v2, sigma=None, mu=None):\n",
        "    if v2:\n",
        "        # In v2, action probability is computed by adjusting the policy weights (M) by a perturbation (nu * delta),\n",
        "        # then scaling by the inverse of the covariance matrix (sigma) and centering by the mean state (mu).\n",
        "        # The resulting value (policy) represents the likelihood of choosing one action over another.\n",
        "        M_plus_delta = M + nu * delta\n",
        "        sigma_sqrt_inv = np.sqrt(1 / np.diag(sigma))\n",
        "        product = M_plus_delta @ np.diag(sigma_sqrt_inv)\n",
        "        diff = state - mu\n",
        "\n",
        "        policy = product @ diff\n",
        "\n",
        "    else:\n",
        "        # The action probability is computed by adjusting the policy weights (M) by a perturbation (nu * delta)\n",
        "        # and then applying these adjusted weights to the current state.\n",
        "        policy = (M + nu * delta) @ state\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "def evaluate_policy(policy, env, max_steps=1000):\n",
        "    state = env.reset()   # Reset the environment to its initial state and get the initial state observation.\n",
        "    total_reward = 0      # Initialize the total accumulated reward to 0.\n",
        "    done = False          # Initialize 'done' to False, indicating the episode hasn't ended.\n",
        "    steps = 0             # Initialize a step counter to track the number of steps taken.\n",
        "    states = []\n",
        "\n",
        "    # Continue taking steps in the environment until the episode ends ('done' becomes True)\n",
        "    # or the number of steps reaches the specified maximum ('max_steps').\n",
        "    while not done and steps < max_steps:\n",
        "        # Determine the action to take in the current state by calling the provided 'policy' function.\n",
        "        action = policy(state)\n",
        "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "            try:\n",
        "                action = 1 if action > 0 else 0\n",
        "            except ValueError as ve:\n",
        "                action = np.argmax(action)\n",
        "        else:\n",
        "            try:\n",
        "                action = np.clip(action, env.action_space.low, env.action_space.high)\n",
        "            except ValueError as ve:\n",
        "                action = np.clip(np.argmax(action), env.action_space.low, env.action_space.high)\n",
        "\n",
        "        state, reward, done, _ = env.step(action)   # Apply the action in the environment, which returns the next state,\n",
        "                                                    # the reward from taking the action, whether the episode has ended,\n",
        "                                                    # and additional info (ignored here with '_').\n",
        "\n",
        "        total_reward += reward  # Add the reward received from the last action to the total accumulated reward.\n",
        "        states.append(state)    # Append the current state to the list\n",
        "        steps += 1              # Increment the step counter.\n",
        "\n",
        "    # Return the total accumulated reward for the episode, which serves as a measure of the policy's performance.\n",
        "    return total_reward, states\n",
        "\n",
        "\n",
        "def augmented_random_search(env, alpha, N, nu, b, max_episodes, v2=False):\n",
        "    # Initialize policy parameters and environment configuration\n",
        "    n = env.observation_space.shape[0]  # Number of features\n",
        "    # Number of actions:\n",
        "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "        p = env.action_space.n\n",
        "    else:\n",
        "        p = env.action_space.shape[0]\n",
        "\n",
        "\n",
        "    M = np.zeros((p, n))    # Initialize policy weights as zeros\n",
        "    mu = np.zeros(n)        # Initialize mean state as zeros\n",
        "    sigma = np.eye(n)       # Initialize covariance matrix as identity matrix\n",
        "\n",
        "    states_encountered = []\n",
        "\n",
        "    # Iterate over a fixed number of episodes to update policy weights\n",
        "    for episode in range(max_episodes):\n",
        "        deltas = np.random.randn(N, p, n)  # Generate N random perturbations for policy weights\n",
        "\n",
        "        rewards_plus = []     # To store rewards when adding perturbations\n",
        "        rewards_minus = []    # To store rewards when subtracting perturbations\n",
        "        states_plus = []\n",
        "        states_minus = []\n",
        "\n",
        "        # Evaluate policy for each perturbation and its negation\n",
        "        for delta in deltas:\n",
        "            # Define perturbed policies\n",
        "            policy_plus = lambda state: policy_function(M, state, delta, nu, v2, sigma, mu)\n",
        "            policy_minus = lambda state: policy_function(M, state, -delta, nu, v2, sigma, mu)\n",
        "\n",
        "            # Evaluate each perturbed policy and store rewards\n",
        "            reward_plus, states_plus_tmp = evaluate_policy(policy_plus, env)\n",
        "            reward_minus, states_minus_tmp = evaluate_policy(policy_minus, env)\n",
        "            rewards_plus.append(reward_plus)\n",
        "            rewards_minus.append(reward_minus)\n",
        "            states_plus.extend(states_plus_tmp)\n",
        "            states_minus.extend(states_minus_tmp)\n",
        "\n",
        "        # Select the top b perturbations based on max rewards obtained from perturbed policies\n",
        "        scores = list(zip(deltas, rewards_plus, rewards_minus))\n",
        "        scores.sort(key=lambda x: max(x[1], x[2]), reverse=True)\n",
        "        top_scores = scores[:b]\n",
        "\n",
        "        # Update policy weights using the top b perturbations\n",
        "        update_step = np.zeros((p, n))\n",
        "        sigma_rewards = np.std([r for _, r_plus, r_minus in top_scores for r in (r_plus, r_minus)]) + 1e-4\n",
        "        for delta, reward_plus, reward_minus in top_scores:\n",
        "            update_step += (reward_plus - reward_minus) * delta\n",
        "\n",
        "        # Apply update to policy weights\n",
        "        M += alpha / (b * sigma_rewards) * update_step\n",
        "\n",
        "        if v2:\n",
        "            # Compute mean and covariance of the encountered states to update the policy parameters\n",
        "            states_encountered.extend(states_plus)\n",
        "            states_encountered.extend(states_minus)\n",
        "            states_array = np.array(states_encountered)\n",
        "            if len(states_array) > 0:\n",
        "                mu = np.mean(states_array, axis=0)\n",
        "                sigma = np.cov(np.array(states_array).T) + 1e-6 * np.eye(n)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return M, mu, sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GcZjuWH5XUa"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "pend_alpha = 0.1\n",
        "pend_N = 10\n",
        "pend_nu = 0.01\n",
        "pend_b = 5\n",
        "pend_max_episodes = 1000\n",
        "\n",
        "# Gym environment\n",
        "env = gym.make('Pendulum-v1')\n",
        "\n",
        "# Run augmented random search\n",
        "M, mu, sigma = augmented_random_search(env, pend_alpha, pend_N, pend_nu, pend_b, pend_max_episodes, v2=False)\n",
        "\n",
        "print(\"Final policy weights (M):\", M)\n",
        "print(\"Final mean state (mu):\", mu)\n",
        "print(\"Final covariance matrix (sigma):\", sigma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drEe7BYG5XUb"
      },
      "outputs": [],
      "source": [
        "M, mu, sigma = augmented_random_search(env, pend_alpha, pend_N, pend_nu, pend_b, pend_max_episodes, v2=True)\n",
        "\n",
        "print(\"Final policy weights (M):\", M)\n",
        "print(\"Final mean state (mu):\", mu)\n",
        "print(\"Final covariance matrix (sigma):\", sigma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKB3rwEV5XUb"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "cp_alpha = 0.1\n",
        "cp_N = 10\n",
        "cp_nu = 0.01\n",
        "cp_b = 5\n",
        "cp_max_episodes = 1000\n",
        "\n",
        "# Gym environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Run augmented random search\n",
        "M, mu, sigma = augmented_random_search(env, cp_alpha, cp_N, cp_nu, cp_b, cp_max_episodes, v2=False)\n",
        "\n",
        "print(\"Final policy weights (M):\", M)\n",
        "print(\"Final mean state (mu):\", mu)\n",
        "print(\"Final covariance matrix (sigma):\", sigma)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "M, mu, sigma = augmented_random_search(env, cp_alpha, cp_N, cp_nu, cp_b, cp_max_episodes, v2=True)\n",
        "\n",
        "print(\"Final policy weights (M):\", M)\n",
        "print(\"Final mean state (mu):\", mu)\n",
        "print(\"Final covariance matrix (sigma):\", sigma)"
      ],
      "metadata": {
        "id": "_t9RpwjW68zh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}