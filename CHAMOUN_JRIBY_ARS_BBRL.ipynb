{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQK9z0LBGm-C",
        "outputId": "40f590fd-5408-4b6c-b6c1-c1de1f047534"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[easypip] Installing bbrl_gymnasium>=0.2.0\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from easypip import easyimport, easyinstall, is_notebook\n",
        "except ModuleNotFoundError as e:\n",
        "    get_ipython().run_line_magic(\"pip\", \"install 'easypip>=1.2.0'\")\n",
        "    from easypip import easyimport, easyinstall, is_notebook\n",
        "\n",
        "# easyinstall(\"swig\")\n",
        "# easyinstall(\"bbrl>=0.2.2\")\n",
        "easyinstall(\"gymnasium\")\n",
        "# easyinstall(\"mazemdp\")\n",
        "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
        "easyinstall(\"tensorboard\")\n",
        "# easyinstall(\"moviepy\")\n",
        "easyinstall(\"box2d-kengz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nC0hUuNaFjPb"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import bbrl\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/osigaud/bbrl.git\n",
        "  import bbrl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MTWBqJk2FlPP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import bbrl_gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JwYVaO5vFnXF"
      },
      "outputs": [],
      "source": [
        "from bbrl.workspace import Workspace\n",
        "from bbrl.agents.agent import Agent\n",
        "from bbrl.agents import Agents, TemporalAgent\n",
        "from bbrl.agents.gyma import AutoResetGymAgent, NoAutoResetGymAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ2paO7mK3Et"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ARSAgent(Agent):\n",
        "    def __init__(self, N, M, nu, b, v2, alpha=0.1, epsilon = 1e-6, sigma=None, mu=None):\n",
        "        \"\"\"\n",
        "        Initialize the ARS (Augmented Random Search) agent.\n",
        "\n",
        "        Args:\n",
        "            N (int): Number of perturbations.\n",
        "            M (torch.tensor): Policy weights.\n",
        "            nu (float): Perturbation magnitude.\n",
        "            b (int): Number of top perturbations to select.\n",
        "            v2 (bool): Flag indicating whether to use ARS version 2.\n",
        "            alpha (float): Learning rate.\n",
        "            epsilon (float): Small value added to the covariance matrix.\n",
        "            sigma (torch.tensor): Covariance matrix.\n",
        "            mu (torch.tensor): Mean state.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.M = torch.tensor(M, dtype=torch.float32)\n",
        "        self.nu = nu\n",
        "        self.b = b\n",
        "        self.v2 = v2\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.sigma = torch.tensor(sigma, dtype=torch.float32) if sigma is not None else torch.eye(M.shape[1])\n",
        "        self.mu = torch.tensor(mu, dtype=torch.float32) if mu is not None else torch.zeros(M.shape[1])\n",
        "        self.delta = torch.zeros_like(self.M)\n",
        "\n",
        "    def forward(self, t, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass of the ARS agent.\n",
        "\n",
        "        Args:\n",
        "            t (int): Time step.\n",
        "            **kwargs: Additional arguments.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        obs = self.get((\"obs\", t))\n",
        "        action = self.policy_function(obs, self.delta)\n",
        "        action = action.view(-1)\n",
        "        action = action.to('cpu')\n",
        "        action = action.float()\n",
        "        self.set((\"action\", t), action)\n",
        "\n",
        "    def policy_function(self, obs, delta):\n",
        "        \"\"\"\n",
        "        Compute the policy function.\n",
        "\n",
        "        Args:\n",
        "            obs (torch.tensor): Observation.\n",
        "            delta (torch.tensor): Perturbation vector.\n",
        "\n",
        "        Returns:\n",
        "            torch.tensor: Action.\n",
        "        \"\"\"\n",
        "        if self.v2:\n",
        "            # In v2, action probability is computed by adjusting the policy weights (M) by a perturbation (nu * delta),\n",
        "            # then scaling by the inverse of the covariance matrix (sigma) and centering by the mean state (mu).\n",
        "            # The resulting value (policy) represents the likelihood of choosing one action over another.\n",
        "            M_plus_delta = self.M + self.nu * delta\n",
        "            sigma_sqrt_inv = torch.linalg.inv(torch.sqrt(self.sigma))\n",
        "            product = torch.mm(M_plus_delta, sigma_sqrt_inv)\n",
        "            diff = obs - self.mu\n",
        "            action = torch.mm(product, diff.unsqueeze(1)).squeeze()\n",
        "        else:\n",
        "            # The action probability is computed by adjusting the policy weights (M) by a perturbation (nu * delta)\n",
        "            # and then applying these adjusted weights to the current state.\n",
        "            action = torch.mm(self.M + self.nu * delta, obs.unsqueeze(1)).squeeze()\n",
        "        return action\n",
        "\n",
        "    def set_delta(self, delta):\n",
        "        \"\"\"\n",
        "        Set the perturbation vector.\n",
        "\n",
        "        Args:\n",
        "            delta (torch.tensor): Perturbation vector.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.delta = torch.tensor(delta, dtype=torch.float32)\n",
        "\n",
        "    def reset_delta(self):\n",
        "        \"\"\"\n",
        "        Reset the perturbation vector.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.delta = torch.zeros_like(self.M)\n",
        "\n",
        "    def update_policy(self, deltas, states_encountered, rewards_plus, rewards_minus):\n",
        "        \"\"\"\n",
        "        Update the policy weights.\n",
        "\n",
        "        Args:\n",
        "            deltas (list): List of perturbation vectors.\n",
        "            states_encountered (list): List of encountered states.\n",
        "            rewards_plus (list): Rewards obtained with positive perturbations.\n",
        "            rewards_minus (list): Rewards obtained with negative perturbations.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        rewards_plus_list = [tensor.item() for tensor in rewards_plus]\n",
        "        rewards_minus_list = [tensor.item() for tensor in rewards_minus]\n",
        "        scores = list(zip(deltas, rewards_plus_list, rewards_minus_list))\n",
        "        scores.sort(key=lambda x: max(x[1], x[2]), reverse=True)\n",
        "        top_scores = scores[:self.b]\n",
        "\n",
        "        # Update policy weights using the top b perturbations\n",
        "        update_step = np.zeros(self.M.shape)\n",
        "        sigma_rewards = np.std([r for _, r_plus, r_minus in top_scores for r in (r_plus, r_minus)]) + self.epsilon\n",
        "        for delta, reward_plus, reward_minus in top_scores:\n",
        "            update_step += (reward_plus - reward_minus) * delta\n",
        "\n",
        "        update_step_tensor = torch.tensor(update_step, dtype=torch.float32)\n",
        "        # Apply update to policy weights\n",
        "        self.M += self.alpha / (self.b * sigma_rewards) * update_step_tensor\n",
        "\n",
        "        # If using ARS V2, update mu and sigma based on states encountered\n",
        "        if self.v2:\n",
        "            states_tensor = torch.tensor(states_encountered)\n",
        "            self.mu = torch.mean(states_tensor, dim=0)\n",
        "            states_tensor_transpose = torch.transpose(states_tensor, 0, 1)\n",
        "            sigma = torch.matmul(states_tensor_transpose, states_tensor) / states_tensor.size(0)\n",
        "\n",
        "            self.sigma += self.epsilon * torch.eye(sigma.size(0))\n",
        "\n",
        "\n",
        "class EnvAgent(Agent):\n",
        "    def __init__(self, gym_env):\n",
        "        \"\"\"\n",
        "        Initialize the environment agent.\n",
        "\n",
        "        Args:\n",
        "            gym_env: Gym environment.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.gym_env = gym_env\n",
        "        # self.states_encountered = [torch.tensor([])]\n",
        "        self.states_encountered = []\n",
        "\n",
        "\n",
        "    def forward(self, t, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass of the environment agent.\n",
        "\n",
        "        Args:\n",
        "            t (int): Time step.\n",
        "            **kwargs: Additional arguments.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        if t==0:\n",
        "            obs = self.gym_env.reset()[0]\n",
        "            # self.states_encountered = obs_tensor\n",
        "            self.set((\"obs\", t), torch.tensor(obs))\n",
        "        else:\n",
        "            action = self.get((\"action\", t-1))\n",
        "            obs, reward, terminated, truncated, _ = self.gym_env.step(action)\n",
        "            self.set((\"obs\", t), torch.tensor(obs))\n",
        "            self.set((\"reward\", t), reward.unsqueeze(0).clone().detach())\n",
        "            # self.set((\"done\", t), torch.tensor(done, dtype=torch.float32))\n",
        "            # obs_tensor = torch.tensor(obs)\n",
        "            # self.states_encountered = torch.cat((self.states_encountered, obs_tensor), dim=0)\n",
        "        self.states_encountered.append(obs)\n",
        "\n",
        "\n",
        "def ars_policy_update(ars_agent, env_agent, t_agent, num_episodes, num_steps_per_episode, workspace):\n",
        "    \"\"\"\n",
        "    Update the ARS policy.\n",
        "\n",
        "    Args:\n",
        "        ars_agent (ARSAgent): ARS agent.\n",
        "        env_agent (EnvAgent): Environment agent.\n",
        "        t_agent (TemporalAgent): Temporal agent.\n",
        "        num_episodes (int): Number of episodes.\n",
        "        num_steps_per_episode (int): Number of steps per episode.\n",
        "        workspace (Workspace): Workspace.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    reward_plus_logs = []\n",
        "    states_encountered = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        deltas = np.random.randn(ars_agent.N, *ars_agent.M.shape)\n",
        "        rewards_plus = []     # To store rewards when adding perturbations\n",
        "        rewards_minus = []    # To store rewards when subtracting perturbations\n",
        "\n",
        "        for delta in deltas:\n",
        "            ars_agent.set_delta(delta)\n",
        "            run_episode(env_agent, t_agent, workspace, num_steps_per_episode)\n",
        "            rewards_plus.append(workspace['reward'].sum())\n",
        "            states_encountered.extend(env_agent.states_encountered)\n",
        "\n",
        "            ars_agent.set_delta(-delta)\n",
        "            run_episode(env_agent, t_agent, workspace, num_steps_per_episode)\n",
        "            rewards_minus.append(workspace['reward'].sum())\n",
        "            states_encountered.extend(env_agent.states_encountered)\n",
        "\n",
        "            ars_agent.reset_delta()\n",
        "\n",
        "        mean_reward = np.mean(np.array(rewards_plus))\n",
        "        reward_plus_logs.append(mean_reward)\n",
        "\n",
        "        ars_agent.update_policy(deltas, states_encountered, rewards_plus, rewards_minus)\n",
        "\n",
        "    plt.plot(range(num_episodes), reward_plus_logs)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def run_episode(env_agent, t_agent, workspace, num_steps):\n",
        "    workspace.clear()\n",
        "    env_agent.gym_env.reset()\n",
        "    t_agent(workspace, t=0, n_steps=num_steps)\n",
        "\n",
        "\n",
        "\n",
        "def run_ars(env_name, N=10, nu=0.03, b=5):\n",
        "    # Initialize the agents and the workspace\n",
        "    env = gym.make(env_name)\n",
        "    env_agent = EnvAgent(env)\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    observation_dim = env.observation_space.shape[0]\n",
        "    M = np.zeros((action_dim, observation_dim))\n",
        "\n",
        "    ars_agent = ARSAgent(N=N, M=M, nu=nu, b=b,v2=True)\n",
        "    composed_agent = Agents(env_agent, ars_agent)\n",
        "    t_agent = TemporalAgent(composed_agent)\n",
        "    workspace = Workspace()\n",
        "\n",
        "    # Run the ARS policy update loop\n",
        "    ars_policy_update(ars_agent, env_agent, t_agent, num_episodes=100, num_steps_per_episode=10, workspace=workspace)\n",
        "\n",
        "\n",
        "run_ars(env_name=\"Pendulum-v1\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
